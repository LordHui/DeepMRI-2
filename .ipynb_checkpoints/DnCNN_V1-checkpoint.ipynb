{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DnCNN_V1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing the \"Beyond Gaussian Denoising\" Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will start by loading two of the images in. Then I will select from the originals each only one. Aftwards, I will select the 500 images in good and bad quality from the two image and create the classification label for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "select = np.random.choice(np.arange(0, 4055), size = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1 = np.load('/home/cloud/MRT_Data/unziped/out/P2_X.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_2 = np.load('/home/cloud/MRT_Data/unziped/out/P11_X.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_3 = np.load('/home/cloud/MRT_Data/unziped/out/P6_X.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X = np.concatenate(seq = (X_1, X_2), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "del X_1, X_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.reshape([4055, 256, 256, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[select, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_1 = np.load('/home/cloud/MRT_Data/unziped/out/P2_Y.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y_2 = np.load('/home/cloud/MRT_Data/unziped/out/P11_Y.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y_3 = np.load('/home/cloud/MRT_Data/unziped/out/P6_Y.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y = np.concatenate(seq = (Y_1, Y_2), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "del Y_1, Y_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = Y_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = Y.reshape([4055, 256, 256, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4055, 256, 256, 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = Y[select, :, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clean up the mess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create train and test and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_frac = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index = int(train_frac * Y.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[0:train_index,:,:,:]\n",
    "X_eval = X[train_index:X.shape[0],:,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = Y[0:train_index,:,:]\n",
    "Y_eval = Y[train_index:X.shape[0],:,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_layer(tensor_in, name_layer, is_training):\n",
    "    x = tf.layers.conv2d(\n",
    "        inputs = tensor_in,\n",
    "        filters = 64,\n",
    "        kernel_size = [3, 3],\n",
    "        padding = \"same\",\n",
    "        activation= None,\n",
    "        name = name_layer)\n",
    "    \n",
    "    x = tf.layers.batch_normalization(x, name = name_layer + \"_bn\",\n",
    "                                             center=True, \n",
    "                                             scale=True, \n",
    "                                             training=is_training)\n",
    "    \n",
    "    return tf.nn.relu(x, name = name_layer + \"_relu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and train CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model_fn(features, labels, mode):\n",
    "    \n",
    "    # Input Layer\n",
    "    input_layer = features['x']\n",
    "    \n",
    "    # Convolutional layer #1     \n",
    "    conv1 = tf.layers.conv2d(\n",
    "        inputs = input_layer,\n",
    "        filters = 64,\n",
    "        kernel_size = 3,\n",
    "        padding = \"same\",\n",
    "        activation= tf.nn.relu,\n",
    "        name = \"Conv_1\")\n",
    "    is_training_mode = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "    \n",
    "     # 18 of the middle layers with Convolution, batch normalization and afterwards ReLu\n",
    "    conv2 = conv_layer(conv1, \"conv2\", is_training = is_training_mode)\n",
    "    conv3 = conv_layer(conv2, \"conv3\", is_training = is_training_mode)\n",
    "    #conv4 = conv_layer(conv3, \"conv4\", is_training = is_training_mode)\n",
    "    #conv5 = conv_layer(conv4, \"conv5\", is_training = is_training_mode)\n",
    "    #conv6 = conv_layer(conv5, \"conv6\", is_training = is_training_mode)\n",
    "    #conv7 = conv_layer(conv6, \"conv7\", is_training = is_training_mode)\n",
    "    #conv8 = conv_layer(conv7, \"conv8\", is_training = is_training_mode)\n",
    "    #conv9 = conv_layer(conv8, \"conv9\", is_training = is_training_mode)\n",
    "    #conv10 = conv_layer(conv9, \"conv10\", is_training = is_training_mode)\n",
    "    #conv11 = conv_layer(conv10, \"conv11\", is_training = is_training_mode)\n",
    "    #conv12 = conv_layer(conv11, \"conv12\", is_training = is_training_mode)\n",
    "    #conv13 = conv_layer(conv12, \"conv13\", is_training = is_training_mode)\n",
    "    #conv14 = conv_layer(conv13, \"conv14\", is_training = is_training_mode)\n",
    "    #conv15 = conv_layer(conv14, \"conv15\", is_training = is_training_mode)\n",
    "    #conv16 = conv_layer(conv15, \"conv16\", is_training = is_training_mode)\n",
    "    #conv17 = conv_layer(conv16, \"conv17\", is_training = is_training_mode)\n",
    "    #conv18 = conv_layer(conv17, \"conv18\", is_training = is_training_mode)\n",
    "    #conv19 = conv_layer(conv18, \"conv19\", is_training = is_training_mode)\n",
    "\n",
    "    # final \n",
    "    final_layer = tf.layers.conv2d(\n",
    "        inputs = conv3,\n",
    "        filters = 1,\n",
    "        kernel_size = [1, 1],\n",
    "        padding = \"same\",\n",
    "        activation = None,\n",
    "        name = \"final_layer\")\n",
    "\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode = mode, predictions=final_layer)\n",
    "    \n",
    "    # Calculate Loss (for both Train and EVAL modes)\n",
    "    # See that the residual learning is implemented here.\n",
    "    loss = tf.losses.mean_squared_error(labels = labels , predictions = final_layer + input_layer)\n",
    "    tf.summary.scalar(\"Value_Loss_Function\", loss)\n",
    "\n",
    "    # Output all learnable variables for tensorboard\n",
    "    for var in tf.trainable_variables():\n",
    "        name = var.name\n",
    "        name = name.replace(':', '_')\n",
    "        tf.summary.histogram(name, var)\n",
    "    merged_summary = tf.summary.merge_all()\n",
    "        \n",
    "    # Configure the Training OP (for TRAIN mode)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        learning_rate = tf.train.exponential_decay(\n",
    "          0.01,                # Base learning rate.\n",
    "          tf.train.get_global_step(),  # Current index into the dataset.\n",
    "          1000,          # Decay step.\n",
    "          0.95,                # Decay rate.\n",
    "          staircase=True)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate)\n",
    "        train_op = optimizer.minimize(loss = loss, global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "        \n",
    "    # Add evaluation metrics\n",
    "    eval_metric_ops = {\n",
    "        \"accuracy\": tf.metrics.mean_squared_error(\n",
    "            labels=labels, predictions=final_layer)}\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "          mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = X_train\n",
    "train_labels = Y_train\n",
    "eval_data = X_eval\n",
    "eval_labels = Y_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "runconf = tf.estimator.RunConfig(save_summary_steps=1, log_step_count_steps = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-02fe82e911ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m ImpNet = tf.estimator.Estimator(config=runconf,\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mmodel_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcnn_model_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/tmp/tmp/test2\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "ImpNet = tf.estimator.Estimator(config=runconf,\n",
    "    model_fn=cnn_model_fn, model_dir=\"/tmp/tmp/test2\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up logging for predictions\n",
    "tensors_to_log = {\"probabilities\": \"softmax_tensor\"}\n",
    "logging_hook = tf.train.LoggingTensorHook(\n",
    "    tensors=tensors_to_log, every_n_iter=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logging_hook = tf.train.SummarySaverHook(\n",
    "     save_steps= 1,\n",
    "    output_dir=\"/home/cloud/Training/summary_hook\", \n",
    "    scaffold=tf.train.Scaffold(summary_op=tf.summary.merge_all())) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": X_train},\n",
    "    y=Y_train,\n",
    "    batch_size=60,\n",
    "    num_epochs=None,\n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ImpNet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-372192b5f525>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m ImpNet.train(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_input_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     steps=20)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ImpNet' is not defined"
     ]
    }
   ],
   "source": [
    "ImpNet.train(\n",
    "    input_fn=train_input_fn,\n",
    "    steps=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what we train for (our label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(Y_eval[1,:,:,0], cmap='gray' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the input of the NN (The noisy image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_eval[1,:,:,0], cmap='gray' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the true image, can be calculated as the difference of X to the label we train on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_eval[1,:,:,0] - Y_eval[1,:,:,0] , cmap='gray' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an original image. What does our network predict?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluate the model and print results\n",
    "predict_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": X_eval[1:2,:,:,:]},\n",
    "    y=X_eval[1:2,:,:,:],\n",
    "    num_epochs=1,\n",
    "    shuffle=False)\n",
    "predict_results = ImpNet.predict(input_fn=predict_input_fn)\n",
    "predict_results = list(predict_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first dictionary is the image shown above. Although I cannot see much in the picture the network is 99.8% sure that it is original!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(predict_results[0][:,:,0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_eval[1:2,:,:,:].reshape([256, 256]) - predict_results[0][:,:,0], cmap='gray')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

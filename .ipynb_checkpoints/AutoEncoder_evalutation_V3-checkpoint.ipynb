{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoEncoderV3 Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1993)\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import gc\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will start by loading two of the images in. Then I will select from the originals each only one. Aftwards, I will select the 500 images in good and bad quality from the two image and create the classification label for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load(\"/scratch2/ttoebro/data/X_test.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16220, 256, 256, 1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.load('/scratch2/ttoebro/data/Y_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16220, 256, 256, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clean up the mess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16220, 256, 256, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_2(tensor_in, name_layer, n_filter, mode, is_start = False):\n",
    "\n",
    "        \n",
    "    x = tf.layers.conv2d(\n",
    "        inputs = tensor_in,\n",
    "        filters = n_filter,\n",
    "        kernel_size = [3, 3],\n",
    "        padding = \"same\",\n",
    "        activation= None,\n",
    "        name = name_layer + \"_conv_1\")\n",
    "    \n",
    "    x = tf.layers.batch_normalization(x,\n",
    "                                      axis = -1,\n",
    "                                      training = (mode == tf.estimator.ModeKeys.TRAIN),\n",
    "                                                  name = name_layer + \"_bn_1\")\n",
    "    x = tf.nn.relu(x, name = name_layer + \"relu_1\")\n",
    "    \n",
    "    x = tf.layers.conv2d(\n",
    "        inputs = x,\n",
    "        filters = n_filter,\n",
    "        kernel_size = [3, 3],\n",
    "        padding = \"same\",\n",
    "        activation= None,\n",
    "        name = name_layer + \"_conv_2\")\n",
    "    \n",
    "    x = tf.layers.batch_normalization(x,\n",
    "                                      axis = -1,\n",
    "                                      training = (mode == tf.estimator.ModeKeys.TRAIN),\n",
    "                                                  name = name_layer + \"_bn_2\")\n",
    "    x = tf.nn.relu(x, name = name_layer + \"relu_2\")\n",
    "    \n",
    "    return x\n",
    "\n",
    "def level_up(tensor_in, insert_layer, name_layer, n_filter, mode):\n",
    "    #print(\"Shape before level up: \" + str(tensor_in.shape))\n",
    "\n",
    "    x = tf.layers.conv2d_transpose(\n",
    "            tensor_in,\n",
    "            filters=n_filter,\n",
    "            kernel_size=2,\n",
    "            strides=2,\n",
    "            padding = 'same',\n",
    "            activation = None,\n",
    "            name=name_layer + \"_upconv\")\n",
    "   # print(\"Shape after level up: \" + str(x.shape))\n",
    "    \n",
    "    x = tf.layers.batch_normalization(x,\n",
    "                                      axis = -1,\n",
    "                                      training = (mode == tf.estimator.ModeKeys.TRAIN),\n",
    "                                                  name = name_layer + \"_bn_1\")\n",
    "    x = tf.nn.relu(x, name_layer + \"relu_1\")\n",
    "    \n",
    "    #print(\"x has dim \" + str(x.shape) + \" and stuff to insert has dim \" + str(insert_layer.shape))\n",
    "    x = tf.concat([insert_layer, x], axis=-1, name=name_layer + \"_insert\")\n",
    "    #print(\"Shape after putting in other vector: \" + str(x.shape))\n",
    "    \n",
    "\n",
    "    x = tf.layers.conv2d(\n",
    "        inputs = x,\n",
    "        filters = n_filter,\n",
    "        kernel_size = [3, 3],\n",
    "        padding = \"same\",\n",
    "        activation= None,\n",
    "        name = name_layer + \"_conv_1\")\n",
    "    \n",
    "    x = tf.layers.batch_normalization(x,\n",
    "                                      axis = -1,\n",
    "                                      training = (mode == tf.estimator.ModeKeys.TRAIN),\n",
    "                                                  name = name_layer + \"_bn_2\")\n",
    "    \n",
    "    x = tf.nn.relu(x, name = name_layer + \"relu_2\")\n",
    "    #print(\"Shape after first conv in level up: \" + str(x.shape))\n",
    "\n",
    "    x = tf.layers.conv2d(\n",
    "        inputs = x,\n",
    "        filters = n_filter,\n",
    "        kernel_size = [3, 3],\n",
    "        padding = \"same\",\n",
    "        activation= None,\n",
    "        name = name_layer + \"_conv_2\")\n",
    "    \n",
    "    x = tf.layers.batch_normalization(x,\n",
    "                                      axis = -1,\n",
    "                                      training = (mode == tf.estimator.ModeKeys.TRAIN),\n",
    "                                                  name = name_layer + \"_bn_3\")\n",
    "    \n",
    "    x = tf.nn.relu(x, name = name_layer + \"relu_3\")\n",
    "    #print(\"Shape after second conv in level up: \" + str(x.shape))\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and train CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AutoEncoder_model(features, labels, mode):\n",
    "    \n",
    "    # Input Tensor\n",
    "    input_tensor = features['x']\n",
    "    \n",
    "    # Level 0\n",
    "    level_0 = conv_2(input_tensor, \"level_0\", n_filter = 64, mode = mode, is_start = True)\n",
    "    level_0_pool = tf.layers.max_pooling2d(level_0, (2, 2), strides=(2, 2), name=\"level_0_pooling\")\n",
    "    \n",
    "    # Level 1\n",
    "    level_1 = conv_2(level_0_pool, \"level_1\", n_filter = 128, mode = mode, is_start = False)\n",
    "    level_1_pool = tf.layers.max_pooling2d(level_1, (2, 2), strides=(2, 2), name=\"level_1_pooling\")\n",
    "    \n",
    "    # Level 2\n",
    "    level_2 = conv_2(level_1_pool, \"level_2\", n_filter = 256, mode = mode, is_start = False)\n",
    "    level_2_pool = tf.layers.max_pooling2d(level_2, (2, 2), strides=(2, 2), name=\"level_2_pooling\")\n",
    "    \n",
    "    # Level 3\n",
    "    level_3 = conv_2(level_2_pool, \"level_3\", n_filter = 512, mode = mode, is_start = False)\n",
    "    level_3_pool = tf.layers.max_pooling2d(level_3, (2, 2), strides=(2, 2), name=\"level_3_pooling\")\n",
    "    \n",
    "    # Level 4\n",
    "    level_4 = conv_2(level_3_pool, \"level_4\", n_filter = 1024, mode = mode, is_start = False)\n",
    "    level_4_pool = tf.layers.max_pooling2d(level_4, (2, 2), strides=(2, 2), name=\"level_4_pooling\")\n",
    "    \n",
    "    # level 5\n",
    "    level_5 = conv_2(level_4_pool, \"level_5\", n_filter = 1024, mode = mode, is_start = False)\n",
    "    level_5_pool = tf.layers.max_pooling2d(level_5, (2, 2), strides=(2, 2), name=\"level_5_pooling\")\n",
    "    \n",
    "    # level 6\n",
    "    level_6 = conv_2(level_5_pool, \"level_6\", n_filter = 1024, mode = mode, is_start = False)\n",
    "    \n",
    "    # level 5\n",
    "    level_5_up = level_up(level_6,level_5,\"level_5_up\" , n_filter = 1024, mode = mode)\n",
    "    \n",
    "    # level 4\n",
    "    level_4_up = level_up(level_5_up,level_4,\"level_4_up\" , n_filter = 1024, mode = mode)\n",
    "    \n",
    "    # Level 3\n",
    "    level_3_up = level_up(level_4_up,level_3,\"level_3_up\" , n_filter = 512, mode = mode)\n",
    "    \n",
    "    # Level 2\n",
    "    level_2_up = level_up(level_3_up,level_2, \"level_2_up\" , n_filter = 256, mode = mode)\n",
    "    \n",
    "    # Level 1\n",
    "    level_1_up = level_up(level_2_up,level_1, \"level_1_up\" , n_filter = 128, mode = mode)\n",
    "    \n",
    "    # Level 0\n",
    "    level_0_up = level_up(level_1_up,level_0,\"level_0_up\"  , n_filter = 64, mode = mode)\n",
    "    \n",
    "    # final \n",
    "    final_layer = tf.layers.conv2d(\n",
    "        inputs = level_0_up,\n",
    "        filters = 1,\n",
    "        kernel_size = [1, 1],\n",
    "        padding = \"same\",\n",
    "        activation = None,\n",
    "        name = \"final_layer\")\n",
    "\n",
    "    \n",
    "    if not (mode == tf.estimator.ModeKeys.PREDICT):\n",
    "        # Output all learnable variables for tensorboard\n",
    "        for var in tf.trainable_variables():\n",
    "            name = var.name\n",
    "            name = name.replace(':', '_')\n",
    "            tf.summary.histogram(name, var)\n",
    "        merged_summary = tf.summary.merge_all()\n",
    "\n",
    "        if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "            tf.summary.image(\"Input_Image\", input_tensor, max_outputs = 1)\n",
    "            tf.summary.image(\"Output_Image\", final_layer, max_outputs = 1)\n",
    "            tf.summary.image(\"True_Image\", labels,  max_outputs = 1)\n",
    "            tf.summary.histogram(\"Summary_final_layer\", final_layer)\n",
    "            tf.summary.histogram(\"Summary_labels\", labels)\n",
    "        \n",
    "    # Give output in prediction mode\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode = mode, predictions=final_layer)\n",
    "    \n",
    "\n",
    "    # Calculate Loss (for both Train and EVAL modes)\n",
    "    # See that the residual learning is implemented here.\n",
    "    loss = tf.losses.mean_squared_error(labels = labels , predictions = final_layer)\n",
    "    tf.summary.scalar(\"Value_Loss_Function\", loss)\n",
    "        \n",
    "    # Configure Learning when training.\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            original_optimizer = tf.train.AdamOptimizer(learning_rate =  0.02)\n",
    "            optimizer = tf.contrib.estimator.clip_gradients_by_norm(original_optimizer, clip_norm=5.0)\n",
    "            train_op = optimizer.minimize(loss = loss, global_step=tf.train.get_global_step())\n",
    "            return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load train and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runconf = tf.estimator.RunConfig(save_summary_steps=5, log_step_count_steps = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': '/home/cloud/model/AutoEncoderV3_2', '_tf_random_seed': None, '_save_summary_steps': 5, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fc4dcc96710>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "AutoEncoder = tf.estimator.Estimator(config=runconf,\n",
    "    model_fn=AutoEncoder_model, model_dir=\"/home/cloud/model/AutoEncoderV3_2\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the sum of the squared and absolute differences between the prediction and the true images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model and print results\n",
    "predict_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": X},\n",
    "    y=Y,\n",
    "    batch_size = 1,\n",
    "    shuffle=False)\n",
    "predict_results = AutoEncoder.predict(input_fn=predict_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_mae = 0\n",
    "sum_mse = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/cloud/model/AutoEncoderV3_2/model.ckpt-989648\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "Current mae is0.017066344618797302\n",
      "Current mse is0.0005869095912203193\n"
     ]
    }
   ],
   "source": [
    "for im_num in range(0, Y.shape[0]):\n",
    "    prediction = next(predict_results)\n",
    "    true_image = Y[im_num,:,:,:]\n",
    "    sum_mae += np.mean(np.abs(prediction - true_image))\n",
    "    sum_mse += np.mean(np.power((prediction - true_image), 2))\n",
    "    if(im_num % 100 == 0):\n",
    "        print(\"Current mae is \" + str(sum_mae) + \" and mse is \" + str(sum_mse) \" at picture \" + str(im_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum_mse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

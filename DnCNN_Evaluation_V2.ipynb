{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building an Autoencoder roughly based on the U-Net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import gc\n",
    "import datetime\n",
    "import skimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.14.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skimage.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will start by loading two of the images in. Then I will select from the originals each only one. Aftwards, I will select the 500 images in good and bad quality from the two image and create the classification label for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load(\"/scratch2/ttoebro/data/X_test.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16220, 256, 256, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.load('/scratch2/ttoebro/data/Y_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16220, 256, 256, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clean up the mess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16220, 256, 256, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_layer(tensor_in, name_layer, is_training):\n",
    "    x = tf.layers.conv2d(\n",
    "    inputs = tensor_in,\n",
    "    filters = 64,\n",
    "    kernel_size = [3, 3],\n",
    "    padding = \"same\",\n",
    "    activation= None,\n",
    "    name = name_layer)\n",
    "    \n",
    "    x = tf.layers.batch_normalization(x, name = name_layer + \"_bn\",\n",
    "                             center=True, \n",
    "                             scale=True, \n",
    "                             training=is_training)\n",
    "    \n",
    "    return tf.nn.relu(x, name = name_layer + \"_relu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and train CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AutoEncoder_model(features, labels, mode):\n",
    "\t# Input Layer\n",
    "\tinput_layer = features['x']\n",
    "\n",
    "\t# Convolutional layer #1     \n",
    "\tconv1 = tf.layers.conv2d(\n",
    "\tinputs = input_layer,\n",
    "\tfilters = 64,\n",
    "\tkernel_size = 3,\n",
    "\tpadding = \"same\",\n",
    "\tactivation= tf.nn.relu,\n",
    "\tname = \"Conv_1\")\n",
    "\tis_training_mode = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "\t# 18 of the middle layers with Convolution, batch normalization and afterwards ReLu\n",
    "\tconv2 = conv_layer(conv1, \"conv2\", is_training = is_training_mode)\n",
    "\tconv3 = conv_layer(conv2, \"conv3\", is_training = is_training_mode)\n",
    "\tconv4 = conv_layer(conv3, \"conv4\", is_training = is_training_mode)\n",
    "\tconv5 = conv_layer(conv4, \"conv5\", is_training = is_training_mode)\n",
    "\tconv6 = conv_layer(conv5, \"conv6\", is_training = is_training_mode)\n",
    "\tconv7 = conv_layer(conv6, \"conv7\", is_training = is_training_mode)\n",
    "\tconv8 = conv_layer(conv7, \"conv8\", is_training = is_training_mode)\n",
    "\tconv9 = conv_layer(conv8, \"conv9\", is_training = is_training_mode)\n",
    "\tconv10 = conv_layer(conv9, \"conv10\", is_training = is_training_mode)\n",
    "\tconv11 = conv_layer(conv10, \"conv11\", is_training = is_training_mode)\n",
    "\tconv12 = conv_layer(conv11, \"conv12\", is_training = is_training_mode)\n",
    "\tconv13 = conv_layer(conv12, \"conv13\", is_training = is_training_mode)\n",
    "\tconv14 = conv_layer(conv13, \"conv14\", is_training = is_training_mode)\n",
    "\tconv15 = conv_layer(conv14, \"conv15\", is_training = is_training_mode)\n",
    "\tconv16 = conv_layer(conv15, \"conv16\", is_training = is_training_mode)\n",
    "\tconv17 = conv_layer(conv16, \"conv17\", is_training = is_training_mode)\n",
    "\tconv18 = conv_layer(conv17, \"conv18\", is_training = is_training_mode)\n",
    "\tconv19 = conv_layer(conv18, \"conv19\", is_training = is_training_mode)\n",
    "\n",
    "\t# final \n",
    "\tfinal_layer = tf.layers.conv2d(\n",
    "\tinputs = conv19,\n",
    "\tfilters = 1,\n",
    "\tkernel_size = [1, 1],\n",
    "\tpadding = \"same\",\n",
    "\tactivation = None,\n",
    "\tname = \"final_layer\") + input_layer\n",
    "\n",
    "\n",
    "\tif not (mode == tf.estimator.ModeKeys.PREDICT):\n",
    "\t# Output all learnable variables for tensorboard\n",
    "\t\tfor var in tf.trainable_variables():\n",
    "\t\t\tname = var.name\n",
    "\t\t\tname = name.replace(':', '_')\n",
    "\t\t\ttf.summary.histogram(name, var)\n",
    "\t\t\tmerged_summary = tf.summary.merge_all()\n",
    "\n",
    "\tif mode == tf.estimator.ModeKeys.TRAIN:\n",
    "\t\ttf.summary.image(\"Input_Image\", input_layer, max_outputs = 1)\n",
    "\t\ttf.summary.image(\"Output_Image\", final_layer, max_outputs = 1)\n",
    "\t\ttf.summary.image(\"True_Image\", labels,  max_outputs = 1)\n",
    "\t\ttf.summary.histogram(\"Summary_final_layer\", final_layer)\n",
    "\t\ttf.summary.histogram(\"Summary_labels\", labels)\n",
    "\n",
    "\t# Give output in prediction mode\n",
    "\tif mode == tf.estimator.ModeKeys.PREDICT:\n",
    "\t\treturn tf.estimator.EstimatorSpec(mode = mode, predictions=final_layer)\n",
    "\n",
    "\n",
    "\t# Calculate Loss (for both Train and EVAL modes)\n",
    "\t# See that the residual learning is implemented here.\n",
    "\tloss = tf.losses.mean_squared_error(labels = labels , predictions = final_layer)\n",
    "\ttf.summary.scalar(\"Value_Loss_Function\", loss)\n",
    "\n",
    "\t# Configure Learning when training.\n",
    "\tif mode == tf.estimator.ModeKeys.TRAIN:\n",
    "\t\tupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\t\twith tf.control_dependencies(update_ops):\n",
    "\t\t\toriginal_optimizer = tf.train.AdamOptimizer(learning_rate =  0.015)\n",
    "\t\t\toptimizer = tf.contrib.estimator.clip_gradients_by_norm(original_optimizer, clip_norm=5.0)\n",
    "\t\t\ttrain_op = optimizer.minimize(loss = loss, global_step=tf.train.get_global_step())\n",
    "\t\t\treturn tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load train and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runconf = tf.estimator.RunConfig(save_summary_steps=5, log_step_count_steps = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': '/home/cloud/model/DnCNN_V2_run3', '_tf_random_seed': None, '_save_summary_steps': 5, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f494730d940>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "AutoEncoder = tf.estimator.Estimator(config=runconf,\n",
    "    model_fn=AutoEncoder_model, model_dir=\"/home/cloud/model/DnCNN_V2_run3\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an original image. What does our network predict?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model and print results\n",
    "predict_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": X[:,:,:,:]},\n",
    "    y=Y[:,:,:,:],\n",
    "    batch_size = 1,\n",
    "    shuffle=False)\n",
    "predict_results = AutoEncoder.predict(input_fn=predict_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_mae = 0\n",
    "sum_mse = 0\n",
    "sum_ssim = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/cloud/model/DnCNN_V2_run3/model.ckpt-811977\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "Current mae is 0.028099406510591507 and mse is 0.001524485764093697 and ssim is 0.8646617021382411 at picture 0\n",
      "Current mae is 14.562550080940127 and mse is 1.1382987378747202 and ssim is 435.1183988859577 at picture 500\n",
      "Current mae is 29.19078677892685 and mse is 2.3109695743769407 and ssim is 870.2021673691427 at picture 1000\n",
      "Current mae is 44.04209250770509 and mse is 3.5821624168893322 and ssim is 1304.0879595044398 at picture 1500\n"
     ]
    }
   ],
   "source": [
    "for im_num in range(0, Y.shape[0]):\n",
    "    prediction = next(predict_results)\n",
    "    true_image = Y[im_num,:,:,:]\n",
    "    sum_mae += np.mean(np.abs(prediction - true_image))\n",
    "    sum_mse += np.mean(np.power((prediction - true_image), 2))\n",
    "    sum_ssim += skimage.measure.compare_ssim(prediction[:,:,0],  true_image[:,:,0])\n",
    "    if(im_num % 500 == 0):\n",
    "        print(\"Current mae is \" + str(sum_mae) + \" and mse is \" + str(sum_mse) + \" and ssim is \" + str(sum_ssim) + \" at picture \" + str(im_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "475.81683526933193\n"
     ]
    }
   ],
   "source": [
    "print(sum_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.21132764022332\n"
     ]
    }
   ],
   "source": [
    "print(sum_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14106.20059513018\n"
     ]
    }
   ],
   "source": [
    "print(sum_ssim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
